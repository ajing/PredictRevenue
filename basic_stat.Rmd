---
title: "basic_stat"
output: html_document
---

Cleaning and some basic operation on data
```{r}
train_data <- read.csv("./Data/train.csv")
test_data  <- read.csv("./Data/test.csv")
train_data$Open.Date = as.Date(train_data$Open.Date, "%m/%d/%Y")
test_data$Open.Date = as.Date(test_data$Open.Date, "%m/%d/%Y")
levels(train_data$Type) = levels(test_data$Type)
levels(train_data$City) = levels(test_data$City)
```

Basic statistics for each variable:
```{r}
summary(train_data)
summary(test_data)
```

boxplot for revenue and P1-P37
```{r}
boxplot(train_data$revenue)
boxplot(x = train_data[, grepl( "P+" , names(train_data))])
```

remove the outlier
```{r}
train_data = subset(train_data, revenue < 1e7)
```

PCA analysis
```{r}
train_pca <- prcomp(train_data[, grepl( "P+" , names(train_data))], center = TRUE, scale = TRUE) 
summary(train_pca)
plot(train_pca)
```

```{r}
biplot(train_pca,choices=c(1,2),cex=0.5, main = "Train data biplot")
PC=predict(train_pca)[,1:3]
summary(PC)
barplot(train_pca$rotation[,"PC1"])
barplot(train_pca$rotation[,"PC2"])
barplot(train_pca$rotation[,"PC3"])
```

```{r}
test_pca <- prcomp(test_data[, grepl( "P+" , names(test_data))], center = TRUE, scale = TRUE) 
summary(test_pca)
plot(test_pca)

biplot(test_pca,choices=c(1,2),cex=0.05, main = "Test data biplot")
PC=predict(test_pca)[,1:3]
summary(PC)
barplot(test_pca$rotation[,"PC1"])
barplot(test_pca$rotation[,"PC2"])
barplot(test_pca$rotation[,"PC3"])
```

Correlation between individuals
```{r, echo=FALSE}
library("gplots")
heatmap.2(as.matrix(train_data[, grepl( "P+" , names(train_data))]), scale="column")
```

```{r}
pairs(~P1+P2+P3+P4+P5+P6+P7+P8,data=train_data)
```

```{r}
PC1=predict(train_pca)[,1]
PC2=predict(train_pca)[,2]
PC3=predict(train_pca)[,3]
model <- lm(revenue ~ PC1 + PC2 + PC3,data = train_data)
summary(model)

model <- lm(revenue ~ P1 + P2 + P3 + P4 + P5 + P6 + P7 + P8 + P9 + P10 + P11 + P12 + P13 + P13 + P14 + P15 + P16 + P17 + P18 + P19 + P20 + P21 + P22 + P23 + P24 + P25 + P26,data = train_data)
summary(model)
```

```{r}
train_data_s = cbind(revenue = train_data[, "revenue"], train_data[, grepl( "P+" , names(train_data))])
model <- lm(revenue ~ .,data = train_data_s)
summary(model)
pre_result_train <- predict(model, train_data_s)
sqrt(sum((pre_result_train - train_data_s$revenue)^2)/ length(train_data$revenue))
pre_result <- predict(model, test_data)
```

SVM
```{r}
library('e1071')
obj <- tune.svm(revenue~., data = subset(train_data, select = -Id), gama = 2^(-10:1), cost = 2^(2:10))
model <- obj$best.model
pre_result_train <- predict(model, subset(train_data,select = Open.Date:P37))
svm_se = sqrt(sum((pre_result_train - train_data$revenue)^2)/ length(train_data$revenue))
svm_se
pre_result <- predict(model, subset(test_data, select = -Id))
```

Backwards feature selection for SVM
```{r}
all_var <- colnames(subset(train_data, select = Open.Date:P37))
iteration <- 10
var_se  <- data.frame(merge(all_var, 1:iteration, all = T), 0)
colnames(var_se) <- c("var_name", "se_ind", "se")

for (variable in all_var){
  se = matrix(0, iteration, 1)
  for (i in 1:iteration){
    train_set <- train_data[, !(colnames(train_data) %in% c("Id", variable))]
    sam_index <- sample.int(nrow(train_set), replace=TRUE)
    obj <- tune.svm(revenue~., data = train_set[sam_index,], gama = 2^(-10:1), cost = 2^(2:10))
    model <- obj$best.model
    pre_result_train <- predict(model, data = train_set[-sam_index,])
    se = sqrt(sum((pre_result_train - train_data$revenue)^2) / length(train_data$revenue))
    var_se[var_se$var_name == variable & var_se$se_ind == i, "se"] <- se
  }
}
```

barplot for each variable
```{r}
library(ggplot2)
library(dplyr)

var_se <- transform(var_se, variable=reorder(variable, -value) ) 
var_se %>%
  group_by(var_name) %>%
  summarise(median = median(se))

ggplot(var_se, aes(x = var_name, y = se)) + geom_boxplot()

var_se$se_median = var_se$se_median - svm_se
var_se$se_max = var_se$se_max - svm_se
var_se$se_min = var_se$se_min - svm_se
ggplot(var_se, aes(x = var_name, y = se_median, ymin = se_min, ymax = se_max)) + geom_pointrange(aes(col = var_name), position=position_dodge(width=0.30))  + ylab("Odds ratio & 95% CI") + geom_hline(aes(yintercept = 1)) + xlab("") + scale_y_log10()

```

only select columns which can decrease standard error
```{r}
backward_se = matrix(0, 20, 1)
model_list  = list()
for (i in 1:20){
  selected <- as.character(var_se[order(-var_se$se),]$var_name)[1:(length(var_se$var_name) - i)]
  obj <- tune.svm(revenue~., data = subset(train_data, select = c("revenue", selected)), gama = 2^(-10:1), cost = 2^(2:10))
  model <- obj$best.model
  pre_result_train <- predict(model, subset(train_data,select = Open.Date:P37))
  backward_se[i] = sqrt(sum((pre_result_train - train_data$revenue)^2))
  model_list[[i]] = model
}
plot(backward_se)
```

The current best SVM model
```{r}
bestcut = which(backward_se == min(backward_se))
model_svm = model_list[[bestcut]]
pre_train_svm <- predict(model_svm, subset(train_data,select = Open.Date:P37))
sqrt(sum((pre_train_svm - train_data$revenue)^2)/ length(train_data$revenue))
```

Bagging (not much improvement for random forest, also not much improvement for SVM)
```{r}
#library(foreach)
#bagging<-function(training,testing,length_divisor=4,iterations=1000)  
#{  
#predictions<-foreach(m=1:iterations,.combine=cbind) %do% {  
#training_positions <- sample(nrow(training), size=floor((nrow(training)/length_divisor)))  
#train_pos<-1:nrow(training) %in% training_positions  
#obj <- tune.svm(revenue~., data = subset(training[train_pos,], select = c("revenue", selected)), gama = 2^(-10:1), cost = 2^(2:10))
#model <- obj$best.model
#predict(model,newdata=testing)  
#}  
#rowMeans(predictions)  
#}
#pre_train_svm<- bagging(train_data, train_data)
#sqrt(sum((pre_train_svm - train_data$revenue)^2))
```


randomForest
```{r}
library("randomForest")
model <- randomForest(revenue~.,data=subset(train_data, select = !(names(train_data) %in% c("Id", "City"))), importance = T, ntree = 200)
pre_result_train <- predict(model, subset(train_data,select = Open.Date:P37))
sqrt(sum((pre_result_train - train_data$revenue)^2)/ length(train_data$revenue))
pre_result <- predict(model, subset(test_data, select = -Id))
```

randomForest with variable selection
```{r}
library("varSelRF")
plot(model)
result <- rfcv(subset(train_data, select = !(names(train_data) %in% c("Id", "City", "revenue"))), train_data$revenue, cv.fold=10, recursive = T)
with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))

plot(model$importance)
hist(model$importance[,"%IncMSE"])

plot(model$importanceSD)
```

Backward selection
```{r}
backward_se = matrix(0, 20, 1)
model_list  = list()
for (i in 1:20){
  selected <- as.character(var_se[order(-var_se$se),]$var_name)[1:(length(var_se$var_name) - i)]
  model <- randomForest(revenue~.,data=subset(train_data, select = names(train_data) %in% c("revenue", selected)))
  pre_result_train <- predict(model, subset(train_data,select = Open.Date:P37))
  backward_se[i] = sqrt(sum((pre_result_train - train_data$revenue)^2)/ length(train_data$revenue))
  model_list[[i]] = model
}
plot(backward_se)
```

Select the model from backward selection
```{r}
bestcut = which(backward_se == min(backward_se))

model_rf = model_list[[bestcut]]
pre_train_forest <- predict(model_rf, subset(train_data,select = Open.Date:P37))
sqrt(sum((pre_train_forest - train_data$revenue)^2) / length(train_data$revenue))

combine_score = data.frame(revenue = train_data$revenue, svm = pre_train_svm, rforest = pre_train_forest)
combine_model = glm(revenue ~ rforest + svm, data = combine_score)

pre_train_combine <- predict(combine_model, combine_score)
sqrt(sum((pre_train_combine - train_data$revenue)^2) / length(train_data$revenue))

pre_test_svm <- predict(model_svm, subset(test_data, select = -Id))
pre_test_forest <- predict(model_rf, subset(test_data, select = -Id))
combine_score = data.frame(svm = pre_test_svm, rforest = pre_test_forest)
pre_result <- predict(combine_model, combine_score)
```


Save prediction to file
```{r}
write.csv(cbind(Id = test_data$Id, Prediction = pre_result), quote = F, row.names = F, file = "./predict_result.csv")
```
