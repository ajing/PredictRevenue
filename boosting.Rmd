---
title: "boosting"
author: "ajing"
date: "04/03/2015"
output: html_document
---

Cleaning and some basic operation on data
```{r}
train_data <- read.csv("./Data/train.csv")
test_data  <- read.csv("./Data/test.csv")
train_data$Open.Date = as.numeric(as.Date(train_data$Open.Date, "%m/%d/%Y"))
test_data$Open.Date = as.numeric(as.Date(test_data$Open.Date, "%m/%d/%Y"))
levels(train_data$Type) = levels(test_data$Type)
levels(train_data$City) = levels(test_data$City)
```

remove the outlier
```{r}
train_data = subset(train_data, revenue < 1e7)
```

Only keep first 5 PCA (sd > 1)
```{r}
train_pca <- prcomp(train_data[, grepl( "P+" , names(train_data))], center = TRUE, scale = TRUE) 
train_lowdim= cbind(train_data[, !grepl( "P+" , names(train_data))], predict(train_pca)[,1:5])
```

Testing set dimension reduction
```{r}
test_lowdim = cbind(test_data[, !grepl( "P+" , names(test_data))], predict(train_pca, newdata = test_data[, grepl( "P+" , names(test_data))])[,1:5])
```

Seperate data set to > 5.1e6 and < 5.1e6
```{r}
train_pca <-  prcomp(train_data[, grepl( "P+" , names(train_data))], center = TRUE, scale = TRUE) 
biplot(train_pca,choices=c(1,2),cex=0.5, main = "Train data biplot", color = c("red", "green")[1+train_data$revenue > 5.1e6])
library(ggbiplot)
ggbiplot(train_pca, groups = train_data$revenue > 5.1e6, ellipse = TRUE)

summary(subset(train_data, revenue > 5.1e6))
summary(subset(train_data, revenue < 5.1e6))
```



randomForest
```{r}
library("randomForest")
train_model <- function(train_lowdim){
model <- randomForest(revenue~.,data=subset(train_lowdim, select = !(names(train_lowdim) %in% c("Id", "City"))), importance = T, ntree = 200, maxnodes = 15)
pre_result_train <- predict(model, subset(train_lowdim,select = Open.Date:PC5))
print(sqrt(sum((pre_result_train - train_lowdim$revenue)^2)/ length(train_lowdim$revenue)))
print(sqrt(sum((pre_result_train - train_lowdim$revenue)^2)/ length(train_lowdim$revenue)))
plot(pre_result_train, train_lowdim$revenue)
model
}
train_model <- train_model(train_lowdim)
train_model_small <- train_model(subset(train_lowdim, revenue <= 5.1e6))
train_model_large <- train_model(subset(train_lowdim, revenue > 5.1e6))

pre_result_s <- predict(train_model_small, subset(test_lowdim, select = -Id))
pre_result_l <- predict(train_model_large, subset(test_lowdim, select = -Id))
plot(pre_result_s, pre_result_l)
hist(train_lowdim$revenue)
hist(c(pre_result_s,pre_result_l))
hist(pre_result_s + pre_result_l)


pre_result[pre_result > 5.1e6] <- predict(train_model_large, subset(test_lowdim, pre_result > 5.1e6, select = -Id))
```

Gradient Boosted Trees

```{r}
library("gbm")
model <- gbm(revenue~.,data=subset(train_data, select = !(names(train_data) %in% c("Id", "City"))), n.trees=1000, interaction.depth=2)
pre_result_train <- predict.gbm(model, subset(train_data,select = Open.Date:P37), type="response", n.trees = 1000)
sqrt(sum((pre_result_train - train_data$revenue)^2)/ length(train_data$revenue))

pre_result <- predict.gbm(model, test_data, type="response", n.trees=1000)
```



Save prediction to file
```{r}
write.csv(cbind(Id = test_data$Id, Prediction = pre_result), quote = F, row.names = F, file = "./predict_result.csv")
```

